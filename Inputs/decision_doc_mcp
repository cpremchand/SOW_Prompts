<context>
deployment of mcp tools server can be of two types one is local deployment(but require python) no cost , another one is cloud server deployment(study of sse and streamable http is required) may need operational cost and custom interafce instead of github copilot.

the mcp tools for word is built to shout actalent formatting it took 4 weeks to complete the 60% maor work ,the remaing 
30 % is unidentified work could caught through use and spotting bugs in the generated word doc

30% is not worked upon right now, we are freezing the code for 60% work done,the 30% bugs or unidentified work will be when the backlog of issues reach upto 10.



to protect the mcp tools for cyber attacks it is better to host in docker instance so that it has access to only mounted folder(local deploymnet)

the content generated by GitHub copilot extension(claude sonnet ai models ) they are working fine with long form document generation.And the other models like GPT are working fine for short form document generation

the accuracy of the gneratef contect by this ai LLM models is mostly dependent on pretraing stage of the ai research lab itself.

	There are other possible ways to increase the accuracy of the generated content

1.Fine tuning (training the model on prioprietry data of real projects)
2.In-contEXT LEARNING(the context window is delicate thing to handle ,giving the LLM a background information of the task it need to complete and applyinga appropriate prompt techquie and prompt stragery(prompt tuning))
3.


The LLM are mostly token processing and generation machines (token is arount 2-3 chars(assumption)) ,the billing is mostly done in token count only like $0.01 for 1Million token (input and output)

billing usually includes:
Input tokens(promts+context(information) +hisstory(optional))
output tokens(LLM models reply)

More text=more tokens =more cost

if for an llm the input tokens is 100 tokens and the model replies  with 400 output tokens  -total is 500 tokens billed.

The Context windodw of the LLM is like the RAM in the cpu ,it is the ammount of text(prompt+context) that it can recollect during generation of response or content

The claude sonnect 4 LLM which we are testing the MCP tools is having a context window around 120k tokens

In present world the highest context window is 10M which is offered by LLama 4 scout(BUILT by Meta)

Like each human having KPIs to measure thier performance(productivity,ROI) ,the LLM Performace(accuracy,reasoning,relevance,hallucinations,tool calling(the abiity to use Mcp tools any other intergrations)) is measured using benchmarks.According to my observation caludesonnet is working fine(closed source).
SOU VERIFY WITH open source models like LLama 4.
 
LLM building is an interative an expensive process it is mostly done by AI research lbs like openai(gpt),anthropic(claude),Alibaba,salesforce,Microsoft(phi 3),google(gemini)

mostly of this High performace LLM models are closed source only avalible thorought API request which require taking a enterprise plan ,and data is sent outside the org for processing(Sod consider th NDA wth clent)

There are many other Moderate open source LLM models(LLama 4 scout(meta),deepseek,Mistral) which can be fine tuned and deployed on organisation hardware(capital is high) or by renting the hardware(capital is lowto nil,operational costs could go from linear to exponential) and require a team to maintance of the AI ecosystem in org.data is reained locally.

Our Researc Projct is mostly ffocused on can use of LLM in moderate task like tech document generation or SOW genration coul reduce the total time of task
like 
1.Manual work(total time=contet generaton(70%) + verification & validation (30%))
2.Use of AI(total time=contet generaton(30%) + verification & validation (40%)) in future with nice LLM models with good capabilities it still could go down

# Technical Analysis: KPI Formulas for Manual vs. AI-Assisted Work

This document provides a comprehensive technical framework for measuring and comparing the efficiency of manual work versus AI-assisted workflows using quantitative metrics and actionable formulas.

---

## 1. Core Time Measurement Formula

The foundation for all efficiency calculations is the total time measurement:

```
Total Time = Generation Time + Validation Time
```

**For Manual Work:**
```
Total Time(Manual) = Generation Time(Manual) + Validation Time(Manual)
```

**For AI-Assisted Work:**
```
Total Time(AI) = Generation Time(AI) + Validation Time(AI)
```

### Key Definitions:
- **Generation Time**: Time spent creating initial content/output
- **Validation Time**: Time spent reviewing, editing, and finalizing content

---

## 2. Time Savings Analysis

### Overall Time Savings Percentage

```
Time Saved (%) = [(Total Time(Manual) - Total Time(AI)) / Total Time(Manual)] × 100
```

**Interpretation:**
- Positive values indicate AI efficiency gains
- Negative values indicate AI overhead costs

### Component-Level Time Savings

**Generation Phase:**
```
Generation Time Saved (%) = [(Gen Time(Manual) - Gen Time(AI)) / Gen Time(Manual)] × 100
```

**Validation Phase:**
```
Validation Time Saved (%) = [(Val Time(Manual) - Val Time(AI)) / Val Time(Manual)] × 100
```

> **Note:** Negative percentages indicate increased time requirements for that phase when using AI.

---

## 3. Productivity Ratio Analysis

Productivity ratios provide intuitive understanding of relative efficiency:

### Total Workflow Efficiency
```
Total Productivity Ratio = Total Time(Manual) / Total Time(AI)
```

### Phase-Specific Ratios
```
Generation Productivity Ratio = Generation Time(Manual) / Generation Time(AI)
Validation Productivity Ratio = Validation Time(Manual) / Validation Time(AI)
```

**Interpretation Scale:**
- **Ratio > 1.0**: AI is faster than manual approach
- **Ratio = 1.0**: AI and manual approaches are equivalent
- **Ratio < 1.0**: AI is slower than manual approach

---

## 4. Practical Example with Real Data

### Sample Dataset
```
Manual Approach:
├── Generation Time: 60 minutes
└── Validation Time: 20 minutes

AI-Assisted Approach:
├── Generation Time: 15 minutes
└── Validation Time: 35 minutes
```

### Step-by-Step Calculations

**1. Total Time Calculation:**
- Manual Total: `60 + 20 = 80 minutes`
- AI Total: `15 + 35 = 50 minutes`

**2. Overall Time Savings:**
```
Time Saved = (80 - 50) / 80 × 100 = 37.5%
```

**3. Phase-Specific Savings:**
- Generation: `(60 - 15) / 60 × 100 = 75% savings`
- Validation: `(20 - 35) / 20 × 100 = -75% (75% increase)`

**4. Productivity Ratios:**
- Total: `80 / 50 = 1.6x faster`
- Generation: `60 / 15 = 4.0x faster`
- Validation: `20 / 35 = 0.57x (1.75x slower)`

---

## 5. Comprehensive Results Summary

| **Metric** | **Manual** | **AI-Assisted** | **Improvement** | **Productivity Ratio** |
|------------|------------|-----------------|-----------------|------------------------|
| **Generation Phase** | 60 min | 15 min | +75% time saved | 4.0x faster |
| **Validation Phase** | 20 min | 35 min | -75% time increase | 0.57x (slower) |
| **Overall Workflow** | 80 min | 50 min | +37.5% time saved | 1.6x faster |

### Key Insights from Example:
- ✅ **Significant generation efficiency**: 4x faster content creation
- ⚠️ **Validation overhead**: 75% more time needed for review/refinement
- ✅ **Net positive outcome**: 37.5% overall time savings despite validation costs

---
---

**This framework provides a robust foundation for quantitative assessment of AI adoption effectiveness and supports data-driven optimization of hybrid workflows.**
---
## Optimization Issues

**1.System hallucinates often?(Solution:Come up with AI Agent Reponse Evaluation(AI Judge) or  Test with Project Members who worked on any previous project)**
Evals tell the reason, maybe i need to provide more data or use better
models or change the design a bit to give lesser autonomy to your agent.

<Possible_Solutions>
Reasoning in LLMs can be elicited and enhanced using many different prompting approaches. Qiao et al. (2023) categorized reasoning methods research into two different branches, namely reasoning enhanced strategy and knowledge enhancement reasoning. Reasoning strategies include prompt engineering, process optimization, and external engines. For instance, single-stage prompting strategies include Chain-of-Thought and Active-Prompt.
keep promting_techniques_to_improve_accuracy.png image here 
</Possible_Solutions>

**2.Unreliable Tool Use?**

LLMs may fail to invoke the right tools or APIs, leading to execution failures should
look into improving tool definitions best practices(use annotations for tools and there parameters)
Or the system has too many tools: Can have a multi-agent system?

<Possible_Solutions>
- Tool transformations for each legacy mcp tool - modern mcp tool
- Parameter description foe each tool 
</Possible_Solutions>
**3.Unreliable retrieval of memory/databases:**
Corrective RAG, (HYDE, Hybrid Search :need knowledge on this new skills),use Agentic RAG
**4.Enable personalization?**
Add Long-Term Memory (Episodic/Procedural) or Temporary short memory is enough or not


Generally, most LLMs have three roles:

- **System Role:** Sets the context and guidelines for the LLM's behavior.
- **User Role:** Represents the user's input, including questions or commands
- **Assistant Role:** The LLM's responses

**System Role:**

- **Purpose:** Sets high-level context, persona, behavioral guidelines, response style, and operational constraints.
- **Functionality:** Establishes the overall scenario and rules for the model.
- **Best Practices:** Focus on general instructions and context setting.

**User Role:**

- **Purpose:** Contains the specific question, task, and contextual information.
- **Functionality:** Provides the details of the task at hand, including examples and desired response structure.
- **Best Practices:** Include specific questions, contextual details, few-shot examples, and response structure guidelines.

**Assistant Role:**

- **Purpose:** Designates the LLM’s response.

**Why Separate Roles?**

- Improved clarity and reduced ambiguity.
- Easier prompt management and iteration.
- Breaking down complex prompts into smaller, more manageable pieces.

**Limitations of a Single Large Prompt:**

- Ambiguity and confusion due to multiple instructions.
- Reduced clarity in how the LLM handles information.
- More difficult to iterate and improve the prompt.
</context>